{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Week 1 - Exercise 2: Model Querying and Inference\n",
    "\n",
    "Objective: Learn to interact with ML models programmatically (reconnaissance skill)\n",
    "\n",
    "INSTRUCTIONS:\n",
    "This script is ~85% complete. Your task is to fill in the TODO sections.\n",
    "Read the hints carefully before implementing each TODO.\n",
    "\n",
    "Red Team Context: Before attacking, you must understand normal model behavior. \n",
    "This is the equivalent of service enumeration in traditional pentesting.\n",
    "\n",
    "You'll query your trained MNIST model to:\n",
    "1. Understand its predictions and confidence scores\n",
    "2. Identify correctly vs incorrectly classified examples\n",
    "3. Analyze decision boundaries\n",
    "4. Document model behavior for exploit development\n",
    "\"\"\"\n"
   ],
   "id": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # STEP 1: LOAD TRAINED MODEL\n"
   ],
   "id": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "print(\"Loading trained MNIST model...\")\n",
    "\n",
    "# Define the same model architecture as in Exercise 1\n",
    "class MNIST_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Load model weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MNIST_CNN().to(device)\n",
    "\n",
    "model_path = Path(__file__).parent.parent.parent / \"models\" / \"mnist_cnn.pt\"\n",
    "\n",
    "# Check if model exists\n",
    "if model_path.exists():\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    print(f\"\u2713 Model loaded from {model_path}\")\n",
    "else:\n",
    "    print(\"\u26a0 Model not found! Please run exercise_1_mnist_classifier.py first\")\n",
    "    exit()\n",
    "\n",
    "model.eval()  # Set to evaluation mode\n",
    "print(f\"Model loaded on device: {device}\")\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # STEP 2: LOAD TEST DATA\n"
   ],
   "id": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "print(\"\\nLoading test dataset...\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "print(f\"Test samples available: {len(test_dataset)}\")\n",
    "\n",
    "# Get a random subset for analysis\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(test_dataset), 100, replace=False)\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # STEP 3: QUERY MODEL WITH TEST SAMPLES\n"
   ],
   "id": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "print(\"\\nQuerying model with test samples...\")\n",
    "\n",
    "predictions = []\n",
    "actual_labels = []\n",
    "confidence_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in sample_indices:\n",
    "        image, label = test_dataset[idx]\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        \n",
    "        # TODO: Get model prediction\n",
    "        # HINT: Call model(image) to get the raw outputs\n",
    "        \n",
    "        # TODO: Apply softmax to get probabilities\n",
    "        # HINT: Use torch.nn.functional.softmax(output, dim=1)\n",
    "        \n",
    "        # TODO: Get predicted class (index with highest probability)\n",
    "        # HINT: Use torch.argmax(output, dim=1).item()\n",
    "        \n",
    "        # TODO: Get confidence score (probability of predicted class)\n",
    "        # HINT: Access probabilities[0][predicted_class].item()\n",
    "        \n",
    "        predictions.append(predicted_class)\n",
    "        actual_labels.append(label)\n",
    "        confidence_scores.append(confidence)\n",
    "\n",
    "# Convert to numpy for easier manipulation\n",
    "predictions = np.array(predictions)\n",
    "actual_labels = np.array(actual_labels)\n",
    "confidence_scores = np.array(confidence_scores)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = (predictions == actual_labels).sum()\n",
    "accuracy = (correct / len(predictions)) * 100\n",
    "print(f\"\\nModel performance on sample:\")\n",
    "print(f\"  Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"  Correct: {correct}/{len(predictions)}\")\n",
    "print(f\"  Average confidence: {confidence_scores.mean():.4f}\")\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # STEP 4: ANALYZE CORRECTLY CLASSIFIED IMAGES\n"
   ],
   "id": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "print(\"\\nAnalyzing correct predictions...\")\n",
    "\n",
    "correct_indices = np.where(predictions == actual_labels)[0]\n",
    "incorrect_indices = np.where(predictions != actual_labels)[0]\n",
    "\n",
    "print(f\"  Correct predictions: {len(correct_indices)}\")\n",
    "print(f\"  Incorrect predictions: {len(incorrect_indices)}\")\n",
    "\n",
    "# Visualize 5 correctly classified examples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle('Correctly Classified Examples', fontsize=16)\n",
    "\n",
    "for i in range(min(5, len(correct_indices))):\n",
    "    idx = sample_indices[correct_indices[i]]\n",
    "    image, label = test_dataset[idx]\n",
    "    pred = predictions[correct_indices[i]]\n",
    "    conf = confidence_scores[correct_indices[i]]\n",
    "    \n",
    "    # Plot original image\n",
    "    axes[0, i].imshow(image.squeeze(), cmap='gray')\n",
    "    axes[0, i].set_title(f'True: {label}, Pred: {pred}\\nConf: {conf:.3f}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Get probability distribution\n",
    "    with torch.no_grad():\n",
    "        img = image.unsqueeze(0).to(device)\n",
    "        output = model(img)\n",
    "        probs = torch.nn.functional.softmax(output, dim=1).cpu().numpy()[0]\n",
    "    \n",
    "    axes[1, i].bar(range(10), probs)\n",
    "    axes[1, i].set_ylim([0, 1])\n",
    "    axes[1, i].set_xlabel('Class')\n",
    "    axes[1, i].set_ylabel('Probability')\n",
    "    axes[1, i].axvline(x=pred, color='r', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('correct_predictions.png', dpi=150)\n",
    "print(\"\\nSaved: correct_predictions.png\")\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # STEP 5: ANALYZE INCORRECTLY CLASSIFIED IMAGES\n"
   ],
   "id": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "print(\"\\nAnalyzing misclassified examples...\")\n",
    "\n",
    "if len(incorrect_indices) > 0:\n",
    "    fig, axes = plt.subplots(2, min(5, len(incorrect_indices)), figsize=(15, 6))\n",
    "    if len(incorrect_indices) == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    fig.suptitle('Misclassified Examples', fontsize=16)\n",
    "    \n",
    "    for i in range(min(5, len(incorrect_indices))):\n",
    "        idx = sample_indices[incorrect_indices[i]]\n",
    "        image, label = test_dataset[idx]\n",
    "        pred = predictions[incorrect_indices[i]]\n",
    "        conf = confidence_scores[incorrect_indices[i]]\n",
    "        \n",
    "        # Plot original image\n",
    "        if len(incorrect_indices) > 1:\n",
    "            axes[0, i].imshow(image.squeeze(), cmap='gray')\n",
    "            axes[0, i].set_title(f'True: {label}, Pred: {pred}\\nConf: {conf:.3f}', \n",
    "                                color='red')\n",
    "            axes[0, i].axis('off')\n",
    "            \n",
    "            # Probability distribution\n",
    "            with torch.no_grad():\n",
    "                img = image.unsqueeze(0).to(device)\n",
    "                output = model(img)\n",
    "                probs = torch.nn.functional.softmax(output, dim=1).cpu().numpy()[0]\n",
    "            \n",
    "            axes[1, i].bar(range(10), probs, color='red')\n",
    "            axes[1, i].set_ylim([0, 1])\n",
    "            axes[1, i].set_xlabel('Class')\n",
    "            axes[1, i].set_ylabel('Probability')\n",
    "            axes[1, i].axvline(x=pred, color='r', linestyle='--', linewidth=2)\n",
    "            axes[1, i].axvline(x=label, color='g', linestyle='--', linewidth=2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('misclassified_examples.png', dpi=150)\n",
    "    print(\"Saved: misclassified_examples.png\")\n",
    "else:\n",
    "    print(\"No misclassified examples found in sample!\")\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # STEP 6: CONFIDENCE DISTRIBUTION ANALYSIS\n"
   ],
   "id": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "print(\"\\nAnalyzing confidence distributions...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Confidence histogram\n",
    "axes[0].hist(confidence_scores, bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(confidence_scores.mean(), color='r', linestyle='--', \n",
    "                label=f'Mean: {confidence_scores.mean():.3f}')\n",
    "axes[0].set_xlabel('Confidence Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Confidence Scores')\n",
    "axes[0].legend()\n",
    "\n",
    "# Accuracy vs Confidence\n",
    "bins = np.linspace(0, 1, 11)\n",
    "bin_indices = np.digitize(confidence_scores, bins)\n",
    "bin_accuracies = []\n",
    "\n",
    "for i in range(1, len(bins)):\n",
    "    mask = bin_indices == i\n",
    "    if mask.sum() > 0:\n",
    "        bin_acc = (predictions[mask] == actual_labels[mask]).mean() * 100\n",
    "        bin_accuracies.append(bin_acc)\n",
    "    else:\n",
    "        bin_accuracies.append(0)\n",
    "\n",
    "axes[1].bar(bins[:-1], bin_accuracies, width=0.1, alpha=0.7, edgecolor='black')\n",
    "axes[1].set_xlabel('Confidence Score Bin')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Accuracy vs Confidence')\n",
    "axes[1].set_ylim([0, 105])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confidence_analysis.png', dpi=150)\n",
    "print(\"Saved: confidence_analysis.png\")\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # STEP 7: CONFUSION MATRIX\n"
   ],
   "id": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "print(\"\\nGenerating confusion matrix...\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(actual_labels, predictions)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "            xticklabels=range(10), yticklabels=range(10))\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=150)\n",
    "print(\"Saved: confusion_matrix.png\")\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # STEP 8: DECISION BOUNDARY ANALYSIS\n"
   ],
   "id": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "print(\"\\nAnalyzing decision boundaries...\")\n",
    "\n",
    "# Analyze which classes are most confused with each other\n",
    "confusion_pairs = []\n",
    "for i in range(len(actual_labels)):\n",
    "    if predictions[i] != actual_labels[i]:\n",
    "        confusion_pairs.append((actual_labels[i], predictions[i]))\n",
    "\n",
    "if confusion_pairs:\n",
    "    from collections import Counter\n",
    "    common_confusions = Counter(confusion_pairs).most_common(5)\n",
    "    \n",
    "    print(\"\\nMost common misclassifications:\")\n",
    "    for (true_label, pred_label), count in common_confusions:\n",
    "        print(f\"  {true_label} \u2192 {pred_label}: {count} times\")\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # STEP 9: DOCUMENTATION\n"
   ],
   "id": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Exercise 2 Complete!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nWhat you accomplished:\")\n",
    "print(\"1. \u2713 Loaded and queried your trained MNIST model\")\n",
    "print(\"2. \u2713 Analyzed prediction confidence scores\")\n",
    "print(\"3. \u2713 Visualized correct and incorrect classifications\")\n",
    "print(\"4. \u2713 Created confusion matrix\")\n",
    "print(\"5. \u2713 Analyzed decision boundaries\")\n",
    "print(\"\\nKey Insights:\")\n",
    "print(f\"  - Overall accuracy on sample: {accuracy:.2f}%\")\n",
    "print(f\"  - Average confidence: {confidence_scores.mean():.4f}\")\n",
    "print(f\"  - High confidence predictions: {(confidence_scores > 0.9).sum()}/{len(confidence_scores)}\")\n",
    "print(\"\\nRed Team Context:\")\n",
    "print(\"  Before attacking a model, understanding its behavior is critical.\")\n",
    "print(\"  This querying/reconnaissance will help you:\")\n",
    "print(\"  - Identify vulnerable inputs (low confidence predictions)\")\n",
    "print(\"  - Understand decision boundaries for adversarial crafting\")\n",
    "print(\"  - Measure attack success (baseline vs adversarial accuracy)\")\n",
    "print(\"\\nNext: Use this knowledge for Week 3 evasion attacks!\")\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}